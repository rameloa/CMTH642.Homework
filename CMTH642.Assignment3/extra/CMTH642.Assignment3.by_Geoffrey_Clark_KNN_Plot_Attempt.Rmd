---
title: "CMTH642 Assignment 3"
author: "Geoffrey Clark"
date: "April 11, 2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Prep

Preparation:  The  dataset is related to white Portuguese "Vinh o Verde" wine. For more info: https://archive.ics.uci.edu/ml/datasets/Wine+Quality

Import to R the following  file: http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv


```{r}
# Download remote content
# wine.df <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", header=T, sep=";")
wine.df <- read.csv("winequality-white.csv", header=T, sep=";")
```

## QUESTIONS

### 1. Check data characteristics. Is there missing data? 
```{r}
sum(is.na(wine.df)) #0
str(wine.df)
sapply(wine.df, function(x) sum(is.na(x))) # 0 NAs
sapply(wine.df, function(x) sum(is.null(x))) # 0 NULLs 
```

### 2. What is the correlation between the attributes other than wine quality? 
```{r}
# For Visualizing Correlation:
# install.packages("corrplot")
library(corrplot)

(wine.cor <- cor(wine.df[,-12]))
corrplot(wine.cor, method="number") # I did this because I appreciate the visual

# highest correlation:
# density & residual.sugar: 0.84
# density & alcohol: -0.78

# I found a cool SO thread on how to do this programmatically:
# https://stackoverflow.com/questions/7074246/show-correlations-as-an-ordered-list-not-as-a-large-matrix#7074856

# as.data.frame(as.table(wine.cor))

# install.packages("reshape")
library(reshape) # includes melt()

wine.cor.list <- wine.cor
wine.cor.list[wine.cor.list == 1] <- NA
wine.cor.list <- na.omit(melt(wine.cor.list))
wine.cor.list[order(-abs(wine.cor.list$value)),]
```


### 3. Graph the frequency distribution of wine quality .
```{r}
hist(wine.df$quality, freq=T, breaks=seq(2.5,9.5,1), main="Frequency Distirbution of Wine Quality", xlab="Wine Quality")
```


### 4. Reduce the levels of rating for quality to three levels as high, medium and low.
```{r}
wine.df$quality <- cut(wine.df$quality, 3, labels=c("Low","Medium","High"))
table(wine.df$quality)
plot(wine.df$quality, main="Distribution of Wine Quality Levels", xlab="Wine Quality Level", ylab="Frequency") # unequal distribution of class variables (potential class imbalance problem)
```


### 5. Normalize the data set.
```{r}
# scale() returns a matrix. Also requires numerical values
# (exclude class attribute (factor): column 12)
wine.df.norm <- as.data.frame(scale(wine.df[,-12], center=T, scale=T)) 
wine.df.norm <- cbind(wine.df.norm, wine.df[,12]) # Re-add class attribute
names(wine.df.norm)[12] <- names(wine.df)[12] # Re-name the class attribute
```

### 6. Divide the data to training and testing groups.
```{r}
# I wanted replicable results for discussion. Remove below call to set.seed() to 
# re-introduce pseudo random numbers in division of test & training set.  
set.seed(42) 

wine.train_index <- sample(1:nrow(wine.df.norm), 0.7*nrow(wine.df.norm))
wine.train <- wine.df.norm[wine.train_index,]
wine.test <- wine.df.norm[-wine.train_index,]
(nrow(wine.df) == (nrow(wine.train) + nrow(wine.test))) # Did we include all observations?
```

### 7. Use the KNN algorithm to predict the quality of wine using its attributes.
```{r}
library(class) # needed for knn()

# I chose to reference the test group directly in the following calls to knn().
# wine.train_labels <- wine.train$quality
# wine.test_labels <- wine.test$quality
#
# Note that subsetting & removing the attribute quality (wine.test[,-12]):
# done to separate class attribute from training & test sets.
# Explicitly defined as cl parameter. 

wine.knn <- list(k3=factor(),k5=factor(),k7=factor())
wine.knn$k3 <- knn(train=wine.train[,-12], test=wine.test[,-12], cl=wine.train$quality, k=3, prob=T)
wine.knn$k5 <- knn(train=wine.train[,-12], test=wine.test[,-12], cl=wine.train$quality, k=5)
wine.knn$k15 <- knn(train=wine.train[-12], test=wine.test[,-12], cl=wine.train$quality, k=15)
```


### 8. Evaluate the model performance .
```{r}
library(gmodels) # Necessary for CrossTable()

# I prefer actual results along the top of the table: I find it easier to read
# This seems pretty standard, and is likewise on Wikipedia: https://en.wikipedia.org/wiki/Confusion_matrix
CrossTable(x=wine.knn$k3, y=wine.test$quality, prop.chisq = F)
CrossTable(x=wine.knn$k5, y=wine.test$quality, prop.chisq = F)
CrossTable(x=wine.knn$k15, y=wine.test$quality, prop.chisq = F)

# Different values of k produce very similar results with the best being a toss up between k=3 & k=5
# I think that overall this model performed acceptably. It would be interesting to compare to other 
# models (Naive Bayes, Decesion Tree, etc) and see how it compares in terms of accuracy. 
#  
# I think another important concept that wasn't included in this assignment is the
# concept of "research question" and "metrics of success". 
# For example, below I listed the True Positive rates for the three levels: low, medium and high.
# However, perhaps True Positives aren't the most important metrics for your research question.
# Maybe False Positives are a bigger issue, or False Negatives. This is context specific but extremely 
# relevant with these types of models. For example, look at the classification of High Quality wines: 
# The TP rates are 37%, 23% and 0% with k set to 3, and 15 respectively. 
# That might not be considered very high depending on your research question.
# 
# Also note the low success TP rate for a high k: 0%. This would seem like a result of the 
# "majority vote" combined with a class imbalance problem. As there are so few Wines in 
# the "high" category it is unlikely that with a higher K, who collects votes from more
# "Nearest Neighbors" there being enough votes in favour of the wine quality being "high". 
# This is definitely an issue to be aware of when using these models.
#
# TP rates (k=3): 64.9%, 78.7%, 37.0% for Low, Medium & High respectively
#
# I also did a Prime Component Analysis (PCA) and ran the kNN model to compare with the above results.
# It produced a very marginal improvement in TP rates for k=3: 65.2%, 78.8%, and 40.0% for Low, Medium
# & High, respectively. 


```


## Extra 
### Visualization
I tried to plot the KNN output of this assignment visually and ran into some trouble
I still believe this to be a great exercise but perhaps not ideal for this particular assignment.

### Some resources:
- https://cran.r-project.org/web/packages/ElemStatLearn/ElemStatLearn.pdf # pg 8: mixture.example examples
- https://stats.stackexchange.com/questions/21572/how-to-plot-decision-boundary-of-a-k-nearest-neighbor-classifier-from-elements-o/21602#21602
- https://stackoverflow.com/questions/31234621/variation-on-how-to-plot-decision-boundary-of-a-k-nearest-neighbor-classifier-f
 
I adapted my code as much as possible from the examples and discussion on Stack Overflow but in the end had to settle for a relatively straightforward cluster plot based on kNN algorithm results. The results of this attempt made me wonder if performing Dimensionality Reduction would help: it didn't. My results are below. In the end I chose to plot pH vs. free.sulfur.dioxide because these are the least correlated factors in the dataset. 
```{r}
require(dplyr)
classif <- wine.knn$k3
prob <- attr(classif, "prob")
wine.gf.df <- bind_rows(mutate(wine.test[,-12],
                               prob=prob,
                               cls="Low",
                               prob_cls=ifelse(classif==cls, 1, 0)),
                        mutate(wine.test[,-12],
                               prob=prob,
                               cls="Medium",
                               prob_cls=ifelse(classif==cls, 1, 0)),
                        mutate(wine.test[,-12],
                               prob=prob,
                               cls="High",
                               prob_cls=ifelse(classif==cls, 1, 0)))

# wine.gf.df.unq <- wine.gf.df[!duplicated(wine.gf.df[, c('pH', 'free.sulfur.dioxide')]), ]

require(ggplot2)
ggplot(wine.gf.df) +
  geom_point(aes(x=pH, y=free.sulfur.dioxide, col=cls),
             data=mutate(wine.test[,-12], cls=classif), 
             size=1.2) +
  # geom_contour(aes(x=pH, y=free.sulfur.dioxide, z=prob_cls, group=cls, color=cols),
  #              bins=2,
  #              data=wine.gf.df.unq) +
  geom_point(aes(x=x, y=y, col=cls),
             size=3,
             data=data.frame(x=wine.train$pH, y=wine.train$free.sulfur.dioxide, cls=wine.train$quality))
```


## PCA 
I decided to spend some more time on this dataset and applied PCA.
My goal was the same as above; to organize the data in such a way to be able to create an effective visual. I was unsuccesful in that regard but did notice that the model performs better, with regards to the TP Rate, than without doing PCA. 

```{r}
wine.pca <- prcomp(wine.df[,-12], scale. = T, center = T)
screeplot(wine.pca, type="lines")


wine.pca.knn <- knn(train=wine.pca$x[wine.train_index,], test=wine.pca$x[-wine.train_index,], cl=wine.df[wine.train_index,'quality'], k=3, prob=T)

CrossTable(x=wine.pca.knn, y=wine.df[-wine.train_index,'quality'], prop.chisq = F)

prob <- attr(wine.pca.knn, "prob")
test <- as.data.frame(wine.pca$x)

wine.pca.gf.df <- bind_rows(mutate(test[-wine.train_index,],
                               prob=prob,
                               cls="Low",
                               prob_cls=ifelse(wine.pca.knn==cls, 1, 0)),
                        mutate(test[-wine.train_index,],
                               prob=prob,
                               cls="Medium",
                               prob_cls=ifelse(wine.pca.knn==cls, 1, 0)),
                        mutate(test[-wine.train_index,],
                               prob=prob,
                               cls="High",
                               prob_cls=ifelse(wine.pca.knn==cls, 1, 0)))


wine.pca.gf.df$cols <- vector(length=nrow(wine.pca.gf.df))
wine.pca.gf.df$cols[wine.pca.gf.df$cls == 'Low'] <- 'c'
wine.pca.gf.df$cols[wine.pca.gf.df$cls == 'Medium'] <- 's'
wine.pca.gf.df$cols[wine.pca.gf.df$cls == 'High'] <- 'v'

ggplot(wine.gf.df) +
  geom_point(aes(x=PC1, y=PC2, col=cls),
             data=mutate(test[-wine.train_index,], cls=wine.pca.knn), 
             size=1.2) +
  # geom_contour(aes(x=PC1, y=PC2, z=prob_cls, group=cls, color=cols),
  #              bins=2,
  #              data=wine.pca.gf.df) +
  geom_point(aes(x=x, y=y, col=cls),
             size=3,
             data=data.frame(x=wine.pca$x[,1], y=wine.pca$x[,2], cls=wine.df$quality))

```

Save your R codes in an RMD file.
Send your  RMD and PDF  files to the course shell.
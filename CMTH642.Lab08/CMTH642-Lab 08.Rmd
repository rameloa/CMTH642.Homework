---
title: "CMTH 642 LAB 08"
output:
  pdf_document: default
  html_document: default
  word_document: default
---


**Question 1**

We are interested to see how variables, such as GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate school at UCLA. The data for this analysis is collected from (http://stats.idre.ucla.edu/).
The admit variable, admit/not admitted, is a binary variable. data can be obtained from our website from within R.

```{r}
mydata <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
## view the first few rows of the data
head(mydata)
```

This dataset has a binary response (outcome, dependent) variable called admit. There are three predictor variables: gre, gpa and rank. We will treat the variables gre and gpa as continuous. The variable rank takes on the values 1 through 4. Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest. We can get basic descriptives for the entire data set by using summary. To get the standard deviations, we use sapply to apply the sd function to each variable in the dataset.

```{r}
summary(mydata)
```

The code below estimates a logistic regression model using the glm (generalized linear model) function. First, we convert rank to a factor to indicate that rank should be treated as a categorical variable.

```{r}
mydata$rank <- factor(mydata$rank)
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
```

**Question:**
How do you analyze the outputs of this model?

**Answer:**

* In the output above, the first thing we see is the call, this is R reminding us what the model we ran was, what options we specified, etc.

* Next we see the deviance residuals, which are a measure of model fit. This part of output shows the distribution of the deviance residuals for individual cases used in the model. Below we discuss how to use summaries of the deviance statistic to assess model fit.

* The next part of the output shows the coefficients, their standard errors, the z-statistic (sometimes called a Wald z-statistic), and the associated p-values. Both gre and gpa are statistically significant, as are the three terms for rank. The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.
    + For every one unit change in GRE, the log odds of admission (versus non-admission) increases by 0.002.
   
    + For a one unit increase in GPA, the log odds of being admitted to graduate school increases by 0.804.
    
    + The indicator variables for rank have a slightly different interpretation. For example, having attended an undergraduate institution with rank of 2, versus an institution with a rank of 1, changes the log odds of admission by -0.675.


**Question 2**

**Plotting logistic regression and classes in R**

Create an imaginary dataset of 20 individuals of different body sizes

```{r}
bodysize=rnorm(20,30,2) # generates 20 values, with mean of 30 & s.d.=2
bodysize=sort(bodysize) # sorts these values in ascending order. 
survive=c(0,0,0,0,0,1,0,1,0,0,1,1,0,1,1,1,0,1,1,1) # assign 'survival' to these 20 individuals non-randomly... most mortality occurs at smaller body size
dat=as.data.frame(cbind(bodysize,survive)) # saves dataframe with two columns: body size & survival
dat
```


Now we can plot using the glm model based on each feature as follows. Here we just have one feature which is Body Size:

```{r}
plot(bodysize,survive,xlab="Body size",ylab="Probability of survival") # plot with body size on x-axis and survival (0 or 1) on y-axis
g=glm(survive~bodysize,family=binomial,dat) # run a logistic regression model (in this case, generalized linear model with logit link). see ?glm

curve(predict(g,data.frame(bodysize=x),type="resp"),add=TRUE) # draws a curve based on prediction from logistic regression model

points(bodysize,fitted(g),pch=20) # optional: you could skip this draws an invisible set of points of body size survival based on a 'fit' to glm model. pch= changes type of dots.
```

**Question 3**

In the "MASS" library there is a data set called "menarche" in which there are three variables: "Age" (average age of age homogeneous groups of girls), "Total" (number of girls in each group), and "Menarche" (number of girls in the group who have reached menarche). 
```{r}
library("MASS") 
data(menarche)
head(menarche)
tail(menarche)
str(menarche)
summary(menarche)
plot(Menarche/Total ~ Age, data=menarche)
glm.out = glm(cbind(Menarche, Total-Menarche) ~ Age, family=binomial(logit), data=menarche) 
summary(glm.out)
plot(Menarche/Total ~ Age, data=menarche)
lines(menarche$Age, glm.out$fitted, type="l", col="red")
title(main="Menarche Data with Fitted Logistic Regression Line") 
```

**Question 4**

Let's analyze the example concerning hypertension. Response is hypertensive y/n.
Predictors are smoking (y/n), obesity (y/n), snoring (y/n). 
How well can these 3 factors explain/predict the presence of hypertension? Which are important? 

We enter the following data:
```{r}
no.yes <- c("No","Yes")
smoking <- gl(2,1,8,no.yes)
obesity <- gl(2,2,8,no.yes)
snoring <- gl(2,4,8,no.yes)
n.tot <- c(60,17,8,2,187,85,51,23)
n.hyp <- c(5,2,1,0,35,13,15,8)
data.frame(smoking,obesity,snoring,n.tot,n.hyp)
```

The `gl` function refers to ?generate levels?. The first three arguments to gl are, respectively, "the number of levels"", "the repeat count of each level", and "the total length of the vector". A fourth
argument can be used to specify the level names of the resulting factor.
The result is apparent from the printout of the generated variables.

The `cbind` function ('c' for 'column') is used to bind variables together,
columnwise, to form a matrix. 
```{r}
hyp.tbl <- cbind(n.hyp,n.tot-n.hyp)
hyp.tbl
```
We can specify the logistic regression model as
```{r}
glm(hyp.tbl~smoking+obesity+snoring,family=binomial("logit"))
```
Actually, "logit" is the default for `binomial` and the `family` argument
is the second argument to `glm`, so it suffices to write
```{r}
glm.hyp <- glm(hyp.tbl~smoking+obesity+snoring,binomial)
```
```{r}
summary(glm.hyp)
```

**Question:**
How are the variables smoking, obesity and snoring correlated? Is it appropriate to remove one of these variables?

**Answer:**
A table of correlations between parameter estimates can be obtained via
the optional argument corr=T to summary (this also works for linear
models). It looks like this:
```{r}
summary(glm.hyp, corr=T)
```
It is seen that the correlation between the estimates is fairly small, so that it may be expected that removing a variable from the model does not change the coefficients and p-values for other variables much. (The correlations between the regression coefficients and intercept are not very informative;
they mostly relate to whether the variable in question has many or few observations in the "Yes" category.)

The z test in the table of regression coefficients immediately shows that the model can be simplified by removing smoking. The result then looks as follows:
```{r}
glm.hyp <- glm(hyp.tbl~obesity+snoring,binomial)
summary(glm.hyp)
```

